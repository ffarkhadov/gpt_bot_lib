"""
report_scripts/p_campain_fin_1.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–ó–∞–ø–æ–ª–Ω—è–µ—Ç —Å—Ç–æ–ª–±–µ—Ü F ¬´–†–∞—Å—Ö–æ–¥—ã –Ω–∞ —Ä–µ–∫–ª–∞–º—É¬ª –≤ unit-day
–ø–æ –¥–∞–Ω–Ω—ã–º Performance API Ozon.

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–∞—è –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π
—Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ª–∏–º–∏—Ç–æ–≤ API –∏ –ø–æ–ª–Ω—ã–º —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö.
"""
from __future__ import annotations

import io
import time
import zipfile
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from typing import Callable

import pandas as pd
import requests
import gspread
from google.oauth2.service_account import Credentials

API = "https://api-performance.ozon.ru"
UTC = timezone.utc

# –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
REQUEST_TIMEOUT = 60
RETRY_DELAY = 70  # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ 429 –æ—à–∏–±–∫–∞—Ö
UUID_CHECK_INTERVAL = 120  # –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ UUID (–∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def log(msg: str):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)


def sleep_progress(sec: int, msg: str = ""):
    if msg:
        log(msg)
    for i in range(sec):
        time.sleep(1)
        print(".", end="", flush=True)
        if (i + 1) % 10 == 0:
            print(f" {i + 1}/{sec}")
    print()


def get_token(session: requests.Session, cid: str, secret: str) -> tuple[str, datetime]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ —Å retry –ª–æ–≥–∏–∫–æ–π"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            r = session.post(
                f"{API}/api/client/token",
                json={"client_id": cid, "client_secret": secret, "grant_type": "client_credentials"},
                headers={"Content-Type": "application/json", "Accept": "application/json"},
                timeout=REQUEST_TIMEOUT,
            )
            if r.status_code == 429:
                sleep_progress(RETRY_DELAY, f"‚ö†Ô∏è 429 –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                continue
            r.raise_for_status()
            token = r.json()["access_token"]
            log(f"‚úÖ –¢–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω ({token[:10]}...)")
            return token, datetime.now(UTC)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            time.sleep(5)


def ensure_token(session: requests.Session,
                 token_time: datetime,
                 cid: str,
                 secret: str,
                 headers_cb: Callable[[dict], None]) -> datetime:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω, –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ >25 –º–∏–Ω (–∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)"""
    if (datetime.now(UTC) - token_time).total_seconds() <= 1500:  # 25 –º–∏–Ω—É—Ç
        return token_time
    log("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏...")
    new_token, new_time = get_token(session, cid, secret)
    headers_cb({"Authorization": f"Bearer {new_token}"})
    return new_time


def chunk(lst: list, n: int = 10):
    """–†–∞–∑–±–∏–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ (–∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ —Ä–∞–±–æ—Ç–∞ —Å Performance API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_campaigns(session: requests.Session, headers: dict) -> list[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞–º–ø–∞–Ω–∏–π —Å retry –ª–æ–≥–∏–∫–æ–π"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            r = session.get(f"{API}/api/client/campaign", headers=headers, timeout=REQUEST_TIMEOUT)
            if r.status_code == 429:
                sleep_progress(RETRY_DELAY, f"‚ö†Ô∏è 429 –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞–º–ø–∞–Ω–∏–π (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                continue
            r.raise_for_status()
            
            # –¢–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∂–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            target_states = {'CAMPAIGN_STATE_RUNNING', 'CAMPAIGN_STATE_STOPPED', 'CAMPAIGN_STATE_INACTIVE'}
            ids = [str(c["id"]) for c in r.json()["list"] if c.get("state") in target_states]
            log(f"üìã –ù–∞–π–¥–µ–Ω–æ –∫–∞–º–ø–∞–Ω–∏–π: {len(ids)} {ids}")
            return ids
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞–º–ø–∞–Ω–∏–π (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            time.sleep(5)


def post_statistics(session: requests.Session,
                    headers: dict,
                    camp_ids: list[str],
                    date_from: str,
                    date_to: str) -> str:
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –ø–æ–ª–Ω–æ–π retry –ª–æ–≥–∏–∫–æ–π"""
    max_retries = 5
    for attempt in range(max_retries):
        try:
            payload = {
                "campaigns": camp_ids,
                "dateFrom": date_from,
                "dateTo": date_to,
                "groupBy": "DATE"  # –¢–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            }
            
            r = session.post(
                f"{API}/api/client/statistics",
                headers=headers,
                json=payload,
                timeout=REQUEST_TIMEOUT
            )
            
            if r.status_code == 429:
                sleep_progress(RETRY_DELAY, f"‚ö†Ô∏è 429 –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                continue
                
            r.raise_for_status()
            uuid = r.json()["UUID"]
            log(f"üì• UUID –ø–æ–ª—É—á–µ–Ω: {uuid}")
            return uuid
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            time.sleep(10)


def wait_uuid(session: requests.Session,
              uuid: str,
              headers_fn: Callable[[], dict],
              refresh_token_fn: Callable[[], None]):
    """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ UUID - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –ª–æ–≥–∏–∫–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    url = f"{API}/api/client/statistics/{uuid}"
    log(f"‚è≥ –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ UUID: {uuid}")
    
    while True:
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            refresh_token_fn()
            
            r = session.get(url, headers=headers_fn(), timeout=REQUEST_TIMEOUT)
            
            if r.status_code == 429:
                sleep_progress(RETRY_DELAY, "‚ö†Ô∏è 429 –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ UUID")
                continue
                
            if r.status_code == 403:
                log("‚ö†Ô∏è 403 –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ UUID - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞")
                refresh_token_fn()
                continue
                
            r.raise_for_status()
            
            data = r.json()
            state = data.get("state")
            log(f"üîç UUID {uuid} ‚Üí state: {state}")
            
            if state == "OK":
                log(f"‚úÖ –û—Ç—á—ë—Ç –≥–æ—Ç–æ–≤: {uuid}")
                return
                
            if state == "FAILED":
                raise RuntimeError(f"‚ùå UUID {uuid} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
                
        except Exception as e:
            log(f"‚ö†Ô∏è –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ UUID {uuid}: {e}")
        
        # –ò–Ω—Ç–µ—Ä–≤–∞–ª –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        time.sleep(UUID_CHECK_INTERVAL)


def download_zip(session: requests.Session, headers: dict, uuid: str) -> bytes:
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ ZIP –æ—Ç—á—ë—Ç–∞ —Å retry –ª–æ–≥–∏–∫–æ–π"""
    max_retries = 5
    for attempt in range(max_retries):
        try:
            r = session.get(
                f"{API}/api/client/statistics/report",
                headers=headers, 
                params={"UUID": uuid}, 
                timeout=REQUEST_TIMEOUT
            )
            
            if r.status_code == 429:
                sleep_progress(RETRY_DELAY, f"‚ö†Ô∏è 429 –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ ZIP (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                continue
                
            if r.status_code == 403:
                log(f"‚ö†Ô∏è 403 –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ ZIP {uuid} - –≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–∫–µ–Ω —É—Å—Ç–∞—Ä–µ–ª")
                raise requests.exceptions.HTTPError("403 Forbidden")
                
            r.raise_for_status()
            
            if "application/zip" not in r.headers.get("Content-Type", ""):
                raise RuntimeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è UUID {uuid}")
                
            log(f"üì¶ ZIP —Å–∫–∞—á–∞–Ω –¥–ª—è UUID {uuid}")
            return r.content
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ZIP (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            time.sleep(10)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CSV ‚Üí DataFrame ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_zip(data: bytes) -> pd.DataFrame:
    """–ü–∞—Ä—Å–∏–Ω–≥ ZIP - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–æ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    all_data = []
    
    try:
        with zipfile.ZipFile(io.BytesIO(data)) as zip_ref:
            for file_name in zip_ref.namelist():
                if not (file_name.endswith('.csv') or file_name.endswith('.txt')):
                    continue
                    
                try:
                    with zip_ref.open(file_name) as f:
                        decoded = io.TextIOWrapper(f, encoding='utf-8')
                        df = pd.read_csv(decoded, sep=';', skiprows=1)
                        
                        # –¢–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                        required_columns = ['–î–µ–Ω—å', 'sku', '–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°']
                        if not all(col in df.columns for col in required_columns):
                            log(f"‚ö†Ô∏è –§–∞–π–ª {file_name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
                            continue
                            
                        df_selected = df[required_columns].copy()
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                        df_selected = df_selected[df_selected['–î–µ–Ω—å'] != '–í—Å–µ–≥–æ']
                        df_selected = df_selected[
                            df_selected['sku'].notna() & 
                            ~df_selected['sku'].isin([float('inf'), -float('inf')])
                        ]
                        
                        if df_selected.empty:
                            continue
                            
                        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                        try:
                            df_selected['sku'] = df_selected['sku'].astype(int)
                        except Exception as e:
                            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è SKU –≤ —Ñ–∞–π–ª–µ {file_name}: {e}")
                            continue
                            
                        all_data.append(df_selected)
                        
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_name}: {e}")
                    continue
                    
    except zipfile.BadZipFile as e:
        log(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π ZIP —Ñ–∞–π–ª: {e}")
        return pd.DataFrame(columns=['date', 'sku', 'rub'])
    
    if not all_data:
        log("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return pd.DataFrame(columns=['date', 'sku', 'rub'])
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    combined_df = pd.concat(all_data, ignore_index=True)
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        combined_df['–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°'] = combined_df['–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°'].replace({',': '.'}, regex=True).astype(float)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–∞ —Ä–∞—Å—Ö–æ–¥–æ–≤: {e}")
        return pd.DataFrame(columns=['date', 'sku', 'rub'])
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    grouped_df = combined_df.groupby(['–î–µ–Ω—å', 'sku'], as_index=False)['–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°'].sum()
    grouped_df.columns = ['date', 'sku', 'rub']
    
    log(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(grouped_df)}")
    return grouped_df


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∑–∞–ø–∏—Å—å –≤ Google Sheets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def write_sheet(gs_cred: str, spread_id: str, sheet_name: str, df: pd.DataFrame):
    """–ó–∞–ø–∏—Å—å –≤ Google Sheets - –ª–æ–≥–∏–∫–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_file(gs_cred, scopes=scope)
        client = gspread.authorize(creds)
        sheet = client.open_by_key(spread_id).worksheet(sheet_name)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        existing_values = sheet.get_all_values()
        if not existing_values:
            log("‚ö†Ô∏è –õ–∏—Å—Ç –ø—É—Å—Ç")
            return
            
        sheet_df = pd.DataFrame(existing_values[1:], columns=existing_values[0])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_sheet_columns = ['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è', 'SKU']
        if not all(col in sheet_df.columns for col in required_sheet_columns):
            raise RuntimeError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ {required_sheet_columns} –≤ Google –¢–∞–±–ª–∏—Ü–µ")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        sheet_df['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'] = sheet_df['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'].apply(
            lambda x: x.split(' ')[0] if isinstance(x, str) and ' ' in x else x
        )
        
        try:
            sheet_df['SKU'] = pd.to_numeric(sheet_df['SKU'], errors='coerce').astype('Int64')
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è SKU: {e}")
            return
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ - —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        sheet_data = sheet_df[['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è', 'SKU']].copy()
        sheet_data['row_index'] = sheet_data.index + 2  # –ò–Ω–¥–µ–∫—Å—ã —Å—Ç—Ä–æ–∫ –≤ Google –¢–∞–±–ª–∏—Ü–µ
        sheet_data['rub'] = None
        
        matches_found = 0
        for _, row in df.iterrows():
            date = row['date']
            sku = row['sku']
            rub = row['rub']
            match = sheet_data[
                (sheet_data['–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è'] == date) & 
                (sheet_data['SKU'] == sku)
            ]
            if not match.empty:
                sheet_data.loc[match.index, 'rub'] = rub
                matches_found += 1
        
        log(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {matches_found}")
        
        # –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–ø–∏—Å—å —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        update_data = [[row['rub'] if row['rub'] is not None else ''] for _, row in sheet_data.iterrows()]
        update_range = f'F2:F{len(sheet_data) + 1}'
        
        sheet.update(range_name=update_range, values=update_data)
        log(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ –≤ Google –¢–∞–±–ª–∏—Ü—É: {matches_found} –∑–Ω–∞—á–µ–Ω–∏–π")
        
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ Google –¢–∞–±–ª–∏—Ü—É: {e}")
        raise


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ run() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run(
    *,
    gs_cred: str,
    spread_id: str,
    sheet_main: str = "unit-day",
    perf_client_id: str,
    perf_client_secret: str,
    days: int = 7,
):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–æ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    log("üöÄ –ó–∞–ø—É—Å–∫ p_campain_fin_1")
    
    session = requests.Session()
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
    token, token_time = get_token(session, perf_client_id, perf_client_secret)
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "Authorization": f"Bearer {token}",
    }
    
    # –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–æ–∫–µ–Ω–æ–º
    def update_headers(new_headers: dict):
        headers.update(new_headers)
    
    def get_headers() -> dict:
        return headers.copy()
    
    def refresh_token():
        nonlocal token_time
        token_time = ensure_token(session, token_time, perf_client_id, perf_client_secret, update_headers)
    
    try:
        # 1. –ü–æ–ª—É—á–∞–µ–º –∫–∞–º–ø–∞–Ω–∏–∏
        campaign_ids = fetch_campaigns(session, get_headers())
        if not campaign_ids:
            log("‚ùå –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π")
            return
        
        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç—ã (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏)
        now_utc = datetime.now(timezone.utc)
        msk_offset = timedelta(hours=3)
        now_msk = now_utc + msk_offset
        date_to = now_msk.date()
        date_from = (now_msk - timedelta(days=days)).date()
        date_from_str = date_from.strftime("%Y-%m-%d")
        date_to_str = date_to.strftime("%Y-%m-%d")
        
        log(f"üìÖ –ü–µ—Ä–∏–æ–¥: {date_from_str} - {date_to_str}")
        
        # 3. –°–æ–±–∏—Ä–∞–µ–º UUID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –∫–∞–º–ø–∞–Ω–∏–π
        uuids = []
        for chunk_campaigns in chunk(campaign_ids, 10):  # 10 –∫–∞–º–ø–∞–Ω–∏–π –≤ —á–∞–Ω–∫–µ –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                refresh_token()
                
                uuid = post_statistics(session, get_headers(), chunk_campaigns, date_from_str, date_to_str)
                
                # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ UUID
                wait_uuid(session, uuid, get_headers, refresh_token)
                
                uuids.append(uuid)
                log(f"‚úÖ UUID {uuid} –≥–æ—Ç–æ–≤ –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é")
                
            except Exception as e:
                log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ –∫–∞–º–ø–∞–Ω–∏–π {chunk_campaigns}: {e}")
                continue
        
        if not uuids:
            log("‚ùå –ù–µ –ø–æ–ª—É—á–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ UUID")
            return
        
        log(f"üìä –í—Å–µ–≥–æ UUID –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {len(uuids)}")
        
        # 4. –°–∫–∞—á–∏–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ ZIP —Ñ–∞–π–ª—ã
        all_dataframes = []
        for uuid in uuids:
            try:
                refresh_token()  # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º
                zip_data = download_zip(session, get_headers(), uuid)
                df = parse_zip(zip_data)
                if not df.empty:
                    all_dataframes.append(df)
                    log(f"‚úÖ UUID {uuid}: –ø–æ–ª—É—á–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
                else:
                    log(f"‚ö†Ô∏è UUID {uuid}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            except Exception as e:
                log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UUID {uuid}: {e}")
                continue
        
        if not all_dataframes:
            log("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏")
            return
        
        # 5. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        final_df = pd.concat(all_dataframes, ignore_index=True)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ –∏ SKU
        final_df = final_df.groupby(['date', 'sku'], as_index=False)['rub'].sum()
        
        log(f"üìä –ò—Ç–æ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏: {len(final_df)}")
        log(f"üí∞ –û–±—â–∞—è —Å—É–º–º–∞ —Ä–∞—Å—Ö–æ–¥–æ–≤: {final_df['rub'].sum():.2f} ‚ÇΩ")
        
        # 6. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ Google Sheets
        write_sheet(gs_cred, spread_id, sheet_main, final_df)
        
        log("‚úÖ p_campain_fin_1 —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω")
        
    except Exception as e:
        log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise
