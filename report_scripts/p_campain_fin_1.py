"""
report_scripts/p_campain_fin_1.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–æ–ª–±–µ—Ü ¬´F = –†–∞—Å—Ö–æ–¥—ã –Ω–∞ —Ä–µ–∫–ª–∞–º—É¬ª –≤ –ª–∏—Å—Ç–µ unit-day
–ø–æ –¥–∞–Ω–Ω—ã–º Performance API Ozon.

run(
    gs_cred            ='/path/sa.json',
    spread_id          ='1AbCdE‚Ä¶',
    sheet_main         ='unit-day',      # –≥–¥–µ –ª–µ–∂–∏—Ç –æ—Ç—á—ë—Ç
    perf_client_id     ='‚Ä¶@advertising.performance.ozon.ru',
    perf_client_secret ='xxxxxxxx',
    days               = 7               # –≥–ª—É–±–∏–Ω–∞ –≤—ã–±–æ—Ä–∫–∏
)
"""
from __future__ import annotations

import io
import time
import zipfile
from datetime import datetime, timedelta, timezone
from collections import defaultdict

import pandas as pd
import requests
import gspread
from google.oauth2.service_account import Credentials

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
API_HOST = "https://api-performance.ozon.ru"
TZ_UTC   = timezone.utc


def log(msg: str):
    """–ú–∏–Ω–∏-–ª–æ–≥–≥–µ—Ä (stdout –ø–æ–ø–∞–¥–∞–µ—Ç –≤ journalctl)."""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)


def backoff_sleep(sec: int):
    """–°–ø–∏—Ç —Å –≤—ã–≤–æ–¥–æ–º —Å–µ–∫—É–Ω–¥–æ–º–µ—Ä–∞ (—á—Ç–æ–±—ã –≤ Telegram –±—ã–ª–æ –≤–∏–¥–Ω–æ ¬´–∂–∏–≤–æ–π¬ª –ª–æ–≥)."""
    for _ in range(sec):
        time.sleep(1)
        print(".", end="", flush=True)
    print()


def get_token(client_id: str, client_secret: str) -> tuple[str, datetime]:
    r = requests.post(
        f"{API_HOST}/api/client/token",
        headers={"Content-Type": "application/json", "Accept": "application/json"},
        json={
            "client_id": client_id,
            "client_secret": client_secret,
            "grant_type": "client_credentials",
        },
        timeout=None,                 # –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Å–∫—Ä–∏–ø—Ç–µ
    )
    r.raise_for_status()
    token = r.json()["access_token"]
    log(f"PerformanceAPI token OK ({token[:10]}‚Ä¶)")

    return token, datetime.now(TZ_UTC)


def chunk(lst: list, size: int = 10):
    for i in range(0, len(lst), size):
        yield lst[i : i + size]


def fetch_campaign_ids(session: requests.Session, headers: dict) -> list[str]:
    r = session.get(f"{API_HOST}/api/client/campaign", headers=headers, timeout=None)
    r.raise_for_status()
    target_states = {
        "CAMPAIGN_STATE_RUNNING",
        "CAMPAIGN_STATE_STOPPED",
        "CAMPAIGN_STATE_INACTIVE",
    }
    ids = [str(c["id"]) for c in r.json()["list"] if c.get("state") in target_states]
    log(f"campaigns: {ids}")
    return ids


def stats_request(
    session: requests.Session,
    headers: dict,
    camp_ids: list[str],
    date_from: str,
    date_to: str,
) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å ‚Üí –ø–æ–ª—É—á–∞–µ–º UUID. –û–±—Ä–∞–±–æ—Ç–∫–∞ 429."""
    while True:
        r = session.post(
            f"{API_HOST}/api/client/statistics",
            headers=headers,
            json={
                "campaigns": camp_ids,
                "dateFrom": date_from,
                "dateTo": date_to,
                "groupBy": "DATE",
            },
            timeout=None,
        )
        if r.status_code == 429:
            log("‚ö†Ô∏è 429 Too Many Requests ‚Äì –∂–¥—ë–º 65 —Å‚Ä¶")
            backoff_sleep(65)
            continue
        r.raise_for_status()
        uuid = r.json()["UUID"]
        return uuid


def wait_uuid(
    session: requests.Session,
    headers_fn,
    uuid: str,
    token_expiry: datetime,
    client_id: str,
    client_secret: str,
    poll_sec: int = 60,
):
    """–û–∂–∏–¥–∞–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ UUID. –ü–µ—Ä–µ–∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    url = f"{API_HOST}/api/client/statistics/{uuid}"
    while True:
        # –∏—Å—Ç—ë–∫ –ª–∏ —Ç–æ–∫–µ–Ω (¬±25 –º–∏–Ω—É—Ç)?
        if (datetime.now(TZ_UTC) - token_expiry).total_seconds() > 1500:
            new_token, token_expiry = get_token(client_id, client_secret)
            headers_fn({"Authorization": f"Bearer {new_token}"})

        r = session.get(url, headers=headers_fn(), timeout=None)
        if r.status_code == 429:
            log("‚ö†Ô∏è 429 –Ω–∞ UUID-check ‚Äì –∂–¥—ë–º 65 —Å‚Ä¶")
            backoff_sleep(65)
            continue
        r.raise_for_status()
        state = r.json()["state"]
        log(f"uuid {uuid} ‚Üí {state}")

        if state == "OK":
            return
        if state == "FAILED":
            raise RuntimeError(f"uuid {uuid} FAILED")

        time.sleep(poll_sec)


def fetch_report_file(session: requests.Session, headers: dict, uuid: str) -> bytes:
    """–°–∫–∞—á–∏–≤–∞–µ–º ZIP-–æ—Ç—á—ë—Ç."""
    r = session.get(
        f"{API_HOST}/api/client/statistics/report",
        headers=headers,
        params={"UUID": uuid},
        timeout=None,
    )
    if r.status_code == 429:
        log("‚ö†Ô∏è 429 –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ ZIP ‚Äì –∂–¥—ë–º 65 —Å‚Ä¶")
        backoff_sleep(65)
        return fetch_report_file(session, headers, uuid)

    r.raise_for_status()
    if "application/zip" not in r.headers.get("Content-Type", ""):
        raise RuntimeError(f"uuid {uuid}: –Ω–µ ZIP")
    return r.content


def parse_zip(data: bytes) -> pd.DataFrame:
    """–ß–∏—Ç–∞–µ–º –≤—Å–µ CSV –≤–Ω—É—Ç—Ä–∏ ZIP, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ date, sku, rub."""
    rows = []
    with zipfile.ZipFile(io.BytesIO(data)) as zf:
        for fn in zf.namelist():
            if not (fn.endswith(".csv") or fn.endswith(".txt")):
                continue
            df = pd.read_csv(
                io.TextIOWrapper(zf.open(fn), "utf-8"),
                sep=";",
                skiprows=1,
                usecols=["–î–µ–Ω—å", "sku", "–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°"],
            )
            df = df[df["–î–µ–Ω—å"] != "–í—Å–µ–≥–æ"]
            df["sku"] = pd.to_numeric(df["sku"], errors="coerce").dropna().astype(int)
            df["–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°"] = (
                df["–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°"].astype(str).str.replace(",", ".").astype(float)
            )
            rows.append(df)

    if not rows:
        return pd.DataFrame(columns=["date", "sku", "rub"])

    df_all = pd.concat(rows, ignore_index=True)
    df_all = (
        df_all.groupby(["–î–µ–Ω—å", "sku"], as_index=False)["–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°"]
        .sum()
        .rename(columns={"–î–µ–Ω—å": "date", "–†–∞—Å—Ö–æ–¥, ‚ÇΩ, —Å –ù–î–°": "rub"})
    )
    return df_all


def write_to_sheet(
    gs_cred: str,
    spread_id: str,
    sheet_name: str,
    df_rub: pd.DataFrame,
):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—É–º–º—ã –≤ —Å—Ç–æ–ª–±–µ—Ü F, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—è (–î–∞—Ç–∞, SKU)."""
    creds = Credentials.from_service_account_file(
        gs_cred,
        scopes=[
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive",
        ],
    )
    ws = (
        gspread.authorize(creds)
        .open_by_key(spread_id)
        .worksheet(sheet_name)
    )

    values = ws.get_all_values()
    if not values:
        log("‚ö†Ô∏è –õ–∏—Å—Ç –ø—É—Å—Ç ‚Äì –Ω–∏—á–µ–≥–æ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å")
        return

    head = values[0]
    try:
        col_date = head.index("–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
        col_sku  = head.index("SKU")
        col_rub  = 5  # —Å—Ç–æ–ª–±–µ—Ü F
    except ValueError as e:
        raise RuntimeError("–í unit-day –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤") from e

    # —Å—Ç—Ä–æ–∏–º –º–∞–ø—É (date, sku) ‚Üí index row_in_sheet
    pos = defaultdict(list)
    for i, row in enumerate(values[1:], start=2):  # —Å—Ç—Ä–æ–∫–∏ –≤ GS ‚Äì —Å 2
        d = (row[col_date].split(" ")[0] if row[col_date] else "", row[col_sku])
        pos[d].append(i)

    updates = []
    for _, r in df_rub.iterrows():
        key = (r["date"], str(r["sku"]))
        for row_idx in pos.get(key, []):
            updates.append(
                {
                    "range": f"F{row_idx}",
                    "values": [[round(r["rub"], 2)]],
                }
            )

    if updates:
        ws.batch_update(updates)
        log(f"üü¢ –ó–∞–ø–∏—Å–∞–Ω–æ {len(updates)} —è—á–µ–µ–∫ –≤ unit-day (—Å—Ç–æ–ª–±–µ—Ü F)")
    else:
        log("‚ÑπÔ∏è –°–æ–≤–ø–∞–¥–µ–Ω–∏–π (–¥–∞—Ç–∞+sku) –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äì –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–ø–∏—Å–∞–Ω–æ")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ run() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run(
    *,
    gs_cred: str,
    spread_id: str,
    sheet_main: str = "unit-day",
    perf_client_id: str,
    perf_client_secret: str,
    days: int = 7,
):
    session = requests.Session()

    # 1. —Ç–æ–∫–µ–Ω
    token, token_time = get_token(perf_client_id, perf_client_secret)
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "Authorization": f"Bearer {token}",
    }

    def headers_proxy(new: dict | None = None):
        """callback –¥–ª—è –ø–æ–¥–º–µ–Ω—ã Authorization –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞"""
        if new:
            headers.update(new)
        return headers

    # 2. –∫–∞–º–ø–∞–Ω–∏–∏
    camp_ids = fetch_campaign_ids(session, headers)

    # 3. –¥–∞—Ç—ã
    now_msk = datetime.now(TZ_UTC) + timedelta(hours=3)
    date_to = now_msk.date()
    date_from = (now_msk - timedelta(days=days)).date()
    df_str, dt_str = date_from.strftime("%Y-%m-%d"), date_to.strftime("%Y-%m-%d")

    # 4. —Å–æ–±–∏—Ä–∞–µ–º UUID-—ã (—Å —É—á—ë—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤)
    all_uuids: list[str] = []
    for ch in chunk(camp_ids, 10):  # –∫–∞–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Å–∫—Ä–∏–ø—Ç–µ
        uuid = stats_request(session, headers, ch, df_str, dt_str)
        log(f"uuid {uuid} OK ‚Üí wait")
        wait_uuid(
            session,
            headers_proxy,
            uuid,
            token_time,
            perf_client_id,
            perf_client_secret,
            poll_sec=60,
        )
        all_uuids.append(uuid)
        time.sleep(1)  # –º—è–≥–∫–∏–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π RPS-–ª–∏–º–∏—Ç

    # 5. —Å–∫–∞—á–∏–≤–∞–µ–º ZIP-—ã –∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º
    df_total = pd.DataFrame(columns=["date", "sku", "rub"])
    for u in all_uuids:
        zip_bytes = fetch_report_file(session, headers, u)
        df = parse_zip(zip_bytes)
        df_total = pd.concat([df_total, df], ignore_index=True)

    if df_total.empty:
        log("‚ö†Ô∏è Performance-–¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äì –≤—ã—Ö–æ–¥")
        return

    # 6. –ø–∏—à–µ–º –≤ Sheets
    write_to_sheet(gs_cred, spread_id, sheet_main, df_total)
    log("‚úÖ p_campain_fin_1 DONE")
